dubbo:
  application:
    name: cluster
  protocol:
    name: dubbo
    port: -1
    host: 172.31.238.102
  registry:
    address: nacos://172.31.238.116:8848
    #address: zookeeper://${zookeeper.address:172.31.226.92}:2181
  consumer:
    timeout: 10000
server:
  port: 8083
hdfs:
  url: hdfs://172.31.238.102:8020
  #url: hdfs://172.31.238.21:8020
  user: bigdata
  prefix: /user/${hdfs.user}/rspmanager/
  origin: ${hdfs.prefix}origin/
  localrsp: ${hdfs.prefix}localrsp/
  globalrsp: ${hdfs.prefix}globalrsp/
  tmp: ${hdfs.prefix}tmp/
  algo: ${hdfs.prefix}algo/
  models: ${hdfs.prefix}models/
  app: ${hdfs.prefix}app/
yarn:
  rm: http://172.31.238.102:8088
spring:
  datasource:
    driver-class-name: com.mysql.jdbc.Driver
    url: jdbc:mysql://172.31.238.97:3306/rspm?allowMultiQueries=true&useSSL=false
    username: rspm
    password: 123456
  #    url: jdbc:mysql://172.31.238.21:3306/rspm?allowMultiQueries=true&useSSL=false
  #    username: root
  #    password: 123456
  task:
    scheduling:
      pool:
        size: 10
      thread-name-prefix: ServiceTimeTask
    execution:
      shutdown:
        #        线程关闭时是否等所有任务结束
        await-termination: false
        #        线程最大的等待时间，防止线程陷入循环关不掉
        await-termination-period: 10s
mybatis:
  mapper-locations: classpath:mapper/*.xml
  type-aliases-package: com.szubd.rsp.pojo
  configuration:
    map-underscore-to-camel-case: true

spark-info:
  clusterConfig:
    clusterName: "Adam"
    yarnResourceManager: "172.31.238.102:8088"
    sparkHistoryServer: "172.31.238.105:18088"
cluster:
  #  cdhApiPath: "172.31.238.97"
  #  cdhApiPassword: "Bdi123456"
  #  rmHostIp: "172.31.238.102"
  #  nnHostIp: "172.31.238.102"
  #  nnHostBackupIp: "172.31.238.105"
  cdhApiPath: "172.31.238.21"
  cdhApiPassword: "bdi123456"
  rmHostIp: "172.31.238.21"
  nnHostIp: "172.31.238.21"
  nnHostBackupIp: "172.31.238.21"
#   cdhApiPath: "172.31.238.99"
#   cdhApiPassword: "admin"
#   rmHostIp: "172.31.238.143"
#   nnHostIp: "172.31.238.143"
#   nnHostBackupIp: "172.31.238.143"

#spark-info:
#  clusterConfig:
#    clusterName: "Gaia"
#    yarnResourceManager: "172.31.238.21:8088"
#    sparkHistoryServer: "172.31.238.21:18088"

#spark-info:
#  clusterConfig:
#    clusterName: "Cluster1"
#    yarnResourceManager: "172.31.238.143:8088"
#    sparkHistoryServer: "172.31.238.143:18088"
